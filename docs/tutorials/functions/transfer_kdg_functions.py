# -*- coding: utf-8 -*-
"""transfer_kdg_functions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yD6P0YDLx4L09ZpO4DTS3ItCkKCgZ7fj
"""

# import modules
import copy
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier as rf 
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
from matplotlib.ticker import ScalarFormatter

from joblib import Parallel, delayed

from kdg import *
from kdg.utils import *

def getNN(dense_size, input_size, levels = 2, n_labels = 2, **kwargs):
    network_base = keras.Sequential()
    network_base.add(layers.Dense(dense_size, activation='relu', input_shape=(input_size,)))
    for l in range(1, levels):
        network_base.add(layers.Dense(dense_size, activation='relu'))
    network_base.add(layers.Dense(units=n_labels, activation = 'softmax'))
    network_base.compile(**kwargs)
    return network_base
    
def get_posteriors(learner, label):
    #define grids
    p = np.arange(-3, 3, step=0.01)
    q = np.arange(-3, 3, step=0.01)
    xx, yy = np.meshgrid(p, -q)
    grid_samples = np.concatenate((xx.reshape(-1, 1), yy.reshape(-1, 1)), axis=1)
    posteriors = learner.predict_proba(grid_samples, label)
    posteriors = posteriors[:,0].reshape(600,600)

    return(posteriors)

def plot_posterior(post, ax = None, cmap="bwr", size = 3):
    if ax is None:
        fig, ax = plt.subplots(1, 1, figsize=(8, 8))

    ax.imshow(
        post,
        extent=[-size, size, -size, size],
        cmap=cmap,
        vmin=0,
        vmax=1,
        interpolation="nearest",
        aspect="auto",
    )

    return ax

def _posterior_figure(X1, y1, X2, y2,
                      post1, post2, t1_acc, t2_acc,
                      post1_trans, post2_trans, t1_acc_trans, t2_acc_trans,
                      label1, label2):
    
    fig1, ax = plt.subplots(2, 3, figsize=(15, 10))

    ax[0,0] = plot_2dsim(X1, y1, ax=ax[0,0])
    ax[1,0] = plot_2dsim(X2, y2, ax=ax[1,0])

    plot_posterior(post1, ax[0,1])
    ax[0,1].set_title(f"{label1} pre-transfer acc={t1_acc}", fontsize=16)
    ax[0,1].set_aspect("equal")

    plot_posterior(post2, ax[1,1])
    ax[1,1].set_title(f"{label2} pre-transfer acc={t2_acc}", fontsize=16)
    ax[1,1].set_aspect("equal")

    ax[0,2] = plot_posterior(post1_trans, ax[0,2])
    ax[0,2].set_title(f"{label1} post-transfer acc={t1_acc_trans}", fontsize=16)
    ax[0,2].set_aspect("equal")

    ax[1,2] = plot_posterior(post2_trans, ax[1,2])
    ax[1,2].set_title(f"{label2} post-transfer acc={t2_acc_trans}", fontsize=16)
    ax[1,2].set_aspect("equal")

def transfer_posteriors(X1, y1, X1_test, y1_test,
                        X2, y2, X2_test, y2_test,
                        label1, label2,
                        learner, **kwargs):
    learner.fit(X1, y1, label1, **kwargs)
    learner.fit(X2, y2, label2, **kwargs)
    post1 = get_posteriors(learner, label1)
    post2 = get_posteriors(learner, label2)
    t1_acc = np.mean(learner.predict(X1_test, label1) == y1_test)
    t2_acc = np.mean(learner.predict(X2_test, label2) == y2_test)

    learner.forward_transfer(X1, y1, label1)
    learner.forward_transfer(X2, y2, label2)

    post1_trans = get_posteriors(learner, label1)
    post2_trans = get_posteriors(learner, label2)
    t1_acc_trans = np.mean(learner.predict(X1_test, label1) == y1_test)
    t2_acc_trans = np.mean(learner.predict(X2_test, label2) == y2_test)

    _posterior_figure(X1, y1, X2, y2,
                      post1, post2, t1_acc, t2_acc,
                      post1_trans, post2_trans, t1_acc_trans, t2_acc_trans,
                      label1, label2)

def force_flip(X, y, X_test, y_test,
               label, learner):
    #divide single array into 2 evenly sized arrays
    if len(np.unique(y)) > 2:
        print("Cannot do flip test on dataset with >2 classes!")
        return

    split = np.random.choice(np.arange(0, len(y)),
                             size=int(len(y)/2),
                             replace=False)
    X1 = X[split, :]
    X2 = np.delete(X, split, axis=0)
    y1 = y[split]
    y2 = -1*(np.delete(y, split)-1)
    labels = [label, f"{label}Flip"]
    y2_test = -1*(y_test-1)

    learner.fit(X1, y1, labels[0])
    learner.fit(X2, y2, labels[1])
    post1 = get_posteriors(learner, labels[0])
    post2 = get_posteriors(learner, labels[1])
    t1_acc = np.mean(learner.predict(X_test, labels[0]) == y_test)
    t2_acc = np.mean(learner.predict(X_test, labels[1]) == y2_test)

    for i in range(2):
        L1 = labels[i]
        L2 = labels[i-1]
        transfer_idx = np.isnan(learner.polytope_sizes[L1])[:,0].nonzero()[0]
        learner.polytope_sizes[L1][transfer_idx] = np.fliplr(learner.polytope_sizes[L2][transfer_idx])

    post1_trans = get_posteriors(learner, labels[0])
    post2_trans = get_posteriors(learner, labels[1])
    t1_acc_trans = np.mean(learner.predict(X_test, labels[0]) == y_test)
    t2_acc_trans = np.mean(learner.predict(X_test, labels[1]) == y2_test)

    _posterior_figure(X1, y1, X2, y2,
                      post1, post2, t1_acc, t2_acc,
                      post1_trans, post2_trans, t1_acc_trans, t2_acc_trans,
                      labels[0], labels[1])

def _reset_learner(learner):
    learner.polytope_means = []
    learner.polytope_cov = []
    learner.polytope_sizes = {}
        
    learner.task_list = []
        
    learner.task_labels = {}
    learner.class_priors = {}
    learner.task_bias = {}
    
    return learner

def run(learner,
        mc_rep,
        n_t1,
        n_t2,
        n_test = 1000,
        gen_1 = generate_spirals,
        gen_2 = None,
        gen_kwargs1 = {'n_class': 2},
        gen_kwargs2 = {'n_class': 3},
        random_state = 100):
    mean_error = np.zeros((6, len(n_t1) + len(n_t2)))
    std_error = np.zeros((6, len(n_t1) + len(n_t2)))
    mean_te = np.zeros((4, len(n_t1) + len(n_t2)))
    std_te = np.zeros((4, len(n_t1) + len(n_t2)))

    if gen_2 is None: gen_2 = gen_1

    task1_id = f"Spiral{n_t1}"
    task2_id = f"Spiral{n_t2}"

    for i, n1 in enumerate(n_t1):
        print(f'starting to compute {n1} {task1_id}')
        error = Parallel(n_jobs = -1)(delayed(experiment)(learner, n1, 0, n_test, task1_id, task2_id, gen_1, gen_2, gen_kwargs1, gen_kwargs2, random_state) for _ in range(mc_rep))
        error = np.array(error)
        mean_error[:, i] = np.mean(error, axis=0)
        std_error[:, i] = np.std(error, ddof=1, axis=0)
        mean_te[0, i] = np.mean(error[:, 0]) / np.mean(error[:, 1])
        mean_te[1, i] = np.mean(error[:, 2]) / np.mean(error[:, 3])
        mean_te[2, i] = np.mean(error[:, 0]) / np.mean(error[:, 4])
        mean_te[3, i] = np.mean(error[:, 2]) / np.mean(error[:, 5])

        if n1 == n_t1[-1]:
            for j, n2 in enumerate(n_t2):
                print(f'starting to compute {n2} {task2_id}')
                error = Parallel(n_jobs = 2)(delayed(experiment)(learner, n1, n2, n_test, task1_id, task2_id, gen_1, gen_2, gen_kwargs1, gen_kwargs2, random_state) for _ in range(mc_rep))
                error = np.array(error)

                mean_error[:, i + j + 1] = np.mean(error, axis=0)
                std_error[:, i + j + 1] = np.std(error, ddof=1, axis=0)
                mean_te[0, i + j + 1] = np.mean(error[:, 0]) / np.mean(error[:, 1])
                mean_te[1, i + j + 1] = np.mean(error[:, 2]) / np.mean(error[:, 3])
                mean_te[2, i + j + 1] = np.mean(error[:, 0]) / np.mean(error[:, 4])
                mean_te[3, i + j + 1] = np.mean(error[:, 2]) / np.mean(error[:, 5])

    return mean_error, std_error, mean_te, std_te


def experiment(
    learner,
    n_task1,
    n_task2,
    n_test,
    task1_id, 
    task2_id,
    gen_1,
    gen_2,
    gen_kwargs1,
    gen_kwargs2,
    random_state
):

    """
    A function to do KDN-FT experiment between a 3-spiral and a 5-spiral
    Parameters
    ----------
    learner : kdn/kdf
        KDF or KDN to use as a template learner for the experiment.
    n_task1 : int
        Total number of train sample for task 1.
    n_task2 : int
        Total number of train dsample for task 2
    n_test : int, optional (default=1000)
        Number of test sample for each task.
    task1_id : str
        Name of Task 1
    task2_id : str
        Name of Task 2
    n_spirals_t1 : int, optional (default=3)
        # classes in task 1.
    n_spirals_t2 : int, optional (default=5)
        # classes in task 2.
    dense_layer_nodes : int, optional (default=10)
        Controls size of neural network
    dense_layers : int, optional (default=3)
        Controls # of layers in neural network (minimum 1)
    random_state : int, RandomState instance, default=None
        Determines random number generation for dataset creation. Pass an int
        for reproducible output across multiple function calls.
    Returns
    -------
    errors : array of shape [6]
        Elements of the array is organized as single task error task1,
        multitask error task1, single task error task2,
        multitask error task2, naive KDN error task1,
        naive KDN task2.
    """

    if n_task1 == 0 and n_task2 == 0:
        raise ValueError("Wake up and provide samples to train!!!")

    if random_state != None:
        np.random.seed(random_state)

    errors = np.zeros(6, dtype=float)

    # generate task1 data
    X_task1, y_task1 = gen_1(n_task1, **gen_kwargs1)
    test_task1, test_label_task1 = gen_1(n_test, **gen_kwargs1)

    #Create KDG learners
    kdg_task1 = _reset_learner(copy.copy(learner))
    kdg_task1.fit(X_task1, y_task1, task_id=task1_id, **gen_kwargs1)

    if n_task2 == 0:
        single_task1 = kdg_task1.predict(test_task1, task_id=task1_id)
        errors[0] = 1 - np.mean(single_task1 == test_label_task1)
        errors[1] = 1 - np.mean(single_task1 == test_label_task1)
        errors[2] = 0.5
        errors[3] = 0.5
        errors[4] = 1 - np.mean(single_task1 == test_label_task1)
        errors[5] = 0.5
    else:
        # generate task2 data
        X_task2, y_task2 = gen_2(n_task2, **gen_kwargs2)
        test_task2, test_label_task2 = gen_2(n_test, **gen_kwargs2)

        kdg_task2 = _reset_learner(copy.copy(learner))
        kdg_task2.fit(X_task2, y_task2, task_id=task2_id, **gen_kwargs2)

        naive_X = np.concatenate((X_task1, X_task2), axis=0)
        naive_y = np.concatenate((y_task1, y_task2), axis=0)
        kdg_naive = _reset_learner(copy.copy(learner))
        kdg_naive.fit(naive_X, naive_y, task_id="Naive", **gen_kwargs1)

        kdg_prog = _reset_learner(copy.copy(learner))

        kdg_prog.fit(X_task1, y_task1, task_id=task1_id, **gen_kwargs1)
        kdg_prog.fit(X_task2, y_task2, task_id=task2_id, **gen_kwargs2)

        kdg_prog.forward_transfer(X_task1, y_task1, task_id=task1_id)
        kdg_prog.forward_transfer(X_task2, y_task2, task_id=task2_id)

        single_task1 = kdg_task1.predict(test_task1, task_id=task1_id)
        single_task2 = kdg_task2.predict(test_task2, task_id=task2_id)
        naive_task1 = kdg_naive.predict(test_task1, task_id="Naive")
        naive_task2 = kdg_naive.predict(test_task2, task_id="Naive")
        prog_task1 = kdg_prog.predict(test_task1, task_id=task1_id)
        prog_task2 = kdg_prog.predict(test_task2, task_id=task2_id)

        errors[0] = 1 - np.mean(single_task1 == test_label_task1)
        errors[1] = 1 - np.mean(prog_task1 == test_label_task1)
        errors[2] = 1 - np.mean(single_task2 == test_label_task2)
        errors[3] = 1 - np.mean(prog_task2 == test_label_task2)
        errors[4] = 1 - np.mean(naive_task1 == test_label_task1)
        errors[5] = 1 - np.mean(naive_task2 == test_label_task2)

    return errors

def plot_error_and_eff(n1s, n2s,
                       mean_error, mean_te,
                       TASK1, TASK2,
                       task1_data, task1_labels,
                       task2_data, task2_labels):
    """
    A function that plots the generalization error and
    transfer efficiency for any experiment
    Parameters
    ----------
    n1s : range(int)
        Array of sample sizes tested for the first learner.
    n2s : range(int)
        Array of sample sizes tested for the second learner.
    mean_error : np.array
        Array of generalization errors.
    mean_te : np.array
        Array of transfer efficiencies.
    TASK1 : str
        String of the name of the first task.
    TASK2 : str
        String of the name of the second task.
    task1_data : np.array
        Example data for training task 1
    task1_labels : np.array
        Labels associated with task1_data
    task2_data : np.array
        Example data for training task 2
    task2_labels : np.array
        Labels associated with task2_data
    """

    ns = np.concatenate((n1s, n2s + n1s[-1]))
    ls = ["-", "--"]

    ################################
    # Plots of Generalization Error
    ################################
    algorithms = [f"Single Task {TASK1}",
                  f"Transfer {TASK1}",
                  f"Single Task {TASK2}",
                  f"Transfer {TASK2}",
                  f"Naive {TASK1}",
                  f"Naive {TASK2}"]

    fontsize = 30
    labelsize = 28

    colors = sns.color_palette("Set1", n_colors=2)

    fig = plt.figure(constrained_layout=True, figsize=(21, 14))
    gs = fig.add_gridspec(14, 21)
    ax1 = fig.add_subplot(gs[7:, :6])

    ax1.plot(
        ns,
        mean_error[1],
        label=algorithms[1],
        c=colors[0],
        ls=ls[np.sum(1 > 1).astype(int)],
        lw=3,
    )

    ax1.plot(
        ns,
        mean_error[0],
        label=algorithms[0],
        c="g",
        ls=ls[np.sum(1 > 1).astype(int)],
        lw=3,
    )

    ax1.set_ylabel("Generalization Error (%s)" % (TASK1), fontsize=fontsize)
    ax1.legend(loc="upper left", fontsize=18, facecolor="white", framealpha=1)
    ax1.set_xlabel("Total Sample Size", fontsize=fontsize)
    ax1.tick_params(labelsize=labelsize)
    #ax1.set_yscale("log")
    ax1.yaxis.set_major_formatter(ScalarFormatter())
    ax1.set_xlim([n1s[0]-10, ns[-1]+10])
    ax1.set_ylim([0, 0.8])
    ax1.set_yticks([0.1, 0.3, 0.5, 0.7])
    ax1.set_xticks([n1s[0], n1s[-1], ns[-1]])
    ax1.axvline(x=n1s[-1], c="gray", linewidth=1.5, linestyle="dashed")
    ax1.set_title(f"{TASK1}       {TASK2}", fontsize=30)

    right_side = ax1.spines["right"]
    right_side.set_visible(False)
    top_side = ax1.spines["top"]
    top_side.set_visible(False)

    #ax1.text(int(0.3*n1s[-1]), 0.15, "%s" % (TASK1), fontsize=26)
    #ax1.text(int(1.4*n1s[-2]), np.mean(ax1.get_ylim()), "%s" % (TASK2), fontsize=26)

    ##############

    ax1 = fig.add_subplot(gs[7:, 7:13])

    ax1.plot(
        ns[len(n1s) :],
        mean_error[2, len(n1s) :],
        label=algorithms[2],
        c="g",
        lw=3
    )

    ax1.plot(
        ns[len(n1s) :],
        mean_error[3, len(n1s) :],
        label=algorithms[3],
        c=colors[0],
        lw=3,
    )

    ax1.set_ylabel("Generalization Error (%s)" % (TASK2), fontsize=fontsize)
    ax1.legend(loc="lower left", fontsize=18, facecolor="white", framealpha=1)
    ax1.set_xlabel("Total Sample Size", fontsize=fontsize)
    ax1.tick_params(labelsize=labelsize)
    #ax1.set_yscale("log")
    ax1.yaxis.set_major_formatter(ScalarFormatter())
    ax1.set_xlim([n1s[0]-10, ns[-1]+10])
    ax1.set_ylim([0, 0.8])
    ax1.set_yticks([0.1, 0.3, 0.5, 0.7])
    ax1.set_xticks([n1s[0], n1s[-1], ns[-1]])
    ax1.axvline(x=n1s[-1], c="gray", linewidth=1.5, linestyle="dashed")
    ax1.set_title(f"{TASK1}       {TASK2}", fontsize=30)

    right_side = ax1.spines["right"]
    right_side.set_visible(False)
    top_side = ax1.spines["top"]
    top_side.set_visible(False)

    #ax1.text(int(0.3*n1s[-1]), 0.2, "%s" % (TASK1), fontsize=26)
    #ax1.text(int(1.4*n1s[-2]), 0.2, "%s" % (TASK2), fontsize=26)

    ax1.set_title(f"{TASK1}       {TASK2}", fontsize=30)

    ################################
    # Plots of Transfer Efficiency
    ################################

    algorithms = ["Transfer BTE", "Transfer  FTE", "Naive BTE", "Naive FTE"]

    ax1 = fig.add_subplot(gs[7:, 14:])

    mean_te = np.log10(mean_te)
    ax1.plot(
        ns[len(n1s) :],
        mean_te[0, len(n1s) :],
        label=algorithms[0],
        c=colors[0],
        ls=ls[0],
        lw=3
    )
    ax1.plot(
        ns[len(n1s) :],
        mean_te[1, len(n1s) :],
        label=algorithms[1],
        c=colors[0],
        ls=ls[1],
        lw=3,
    )
    ax1.plot(
        ns,
        mean_te[2],
        label=algorithms[2],
        c="g",
        ls=ls[0],
        lw=3
    )
    ax1.plot(
        ns[len(n1s) :],
        mean_te[3, len(n1s) :],
        label=algorithms[3],
        c="g",
        ls=ls[1],
        lw=3,
    )

    ax1.set_ylabel(
        "log Forward/Backward \n Transfer Efficiency (FTE/BTE)", fontsize=fontsize
    )
    ax1.legend(loc="lower left", fontsize=18, framealpha=1)
    ax1.set_xlabel("Total Sample Size", fontsize=fontsize)

    log_lbl = [-0.4, -0.2, 0, 0.2, 0.4]
    ax1.set_yticks(log_lbl)

    ax1.set_title(f"{TASK1}       {TASK2}", fontsize=30)
    
    ax1.tick_params(labelsize=labelsize)
    ax1.set_xticks([n1s[0], n1s[-1], ns[-1]])
    ax1.axvline(x=n1s[-1], c="gray", linewidth=1.5, linestyle="dashed")
    right_side = ax1.spines["right"]
    right_side.set_visible(False)
    top_side = ax1.spines["top"]
    top_side.set_visible(False)
    ax1.hlines(0, 50, 1500, colors="gray", linestyles="dashed", linewidth=1.5)

    #ax1.text(int(0.3*n1s[-1]), np.mean(ax1.get_ylim()), "%s" % (TASK1), fontsize=26)
    #ax1.text(int(1.4*n1s[-2]), np.mean(ax1.get_ylim()), "%s" % (TASK2), fontsize=26)

    ##############

    n_colors = max(len(np.unique(task1_labels)), len(np.unique(task2_labels)))
    colors = sns.color_palette("Dark2", n_colors=n_colors)

    ax = fig.add_subplot(gs[:6, 3:10])
    clr = [colors[i] for i in task1_labels]
    ax.scatter(task1_data[:, 0], task1_data[:, 1], c=clr, s=50)

    ax.set_xticks([])
    ax.set_yticks([])
    ax.set_title(TASK1, fontsize=30)

    ax.axis("off")

    ax = fig.add_subplot(gs[:6, 11:17])
    clr = [colors[i] for i in task2_labels]
    ax.scatter(task2_data[:, 0], task2_data[:, 1], c=clr, s=50)

    ax.set_xticks([])
    ax.set_yticks([])
    ax.set_title(TASK2, fontsize=30)
    ax.axis("off")